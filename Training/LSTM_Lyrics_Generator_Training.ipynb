{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"gNZLzS0Xdopu"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import re\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from collections import Counter\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.autograd import Variable\n","import time\n","torch.cuda.is_available()\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_xIF0REtJdcu"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jjvd7FmmdsSW","outputId":"933f2047-7f19-4396-ab68-e0f17d904db3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-a7JBpmsdopz","outputId":"ee038842-d5db-4411-8b2b-9a86e3647890"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=22c00e286b38d73ce931b43d735d6f61f2dde35005834af618ca85b6a64615e7\n","  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n","Successfully built langdetect\n","Installing collected packages: langdetect\n","Successfully installed langdetect-1.0.9\n"]}],"source":["!pip install langdetect"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4iIosH7Xdopz"},"outputs":[],"source":["from langdetect import detect"]},{"cell_type":"markdown","metadata":{"id":"nPZfpjwldop0"},"source":["## Load the Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"19DEparjdop1"},"outputs":[],"source":["def filter_english_songs(chunk):\n","    english_songs = []\n","    for index, row in chunk.iterrows():\n","        try:\n","            if detect(row['lyrics']) == 'en':\n","                english_songs.append(row)\n","        except:\n","            pass\n","    return pd.DataFrame(english_songs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5NzqGpRCdop2"},"outputs":[],"source":["# Load the dataset in chunks\n","chunk_size = 10000  # Adjust chunk size based on available memory\n","songs_number = 5000\n","\n","def sample_songs_from_chunks(data_chunks, genre, songs_number=songs_number):\n","\n","    sampled_songs = pd.DataFrame()\n","\n","    # Filter English songs and sample specified number of songs\n","    for chunk in data_chunks:\n","        data_by_genre = chunk[['lyrics', 'tag']][chunk['tag'] == genre]\n","        data_by_genre_english = filter_english_songs(data_by_genre)\n","        sampled_songs = pd.concat([sampled_songs, data_by_genre_english])\n","        if len(sampled_songs) >= songs_number:\n","            break\n","\n","    # Sample specified number of songs\n","    sampled_songs = sampled_songs.sample(n=songs_number, random_state=42)\n","    return sampled_songs\n","\n","# Read dataset in chunks\n","file_path = '/content/drive/MyDrive/Final_Project/Data_set/ds2.csv'\n","\n","data_chunks = pd.read_csv(file_path, chunksize=chunk_size)\n","rap_songs = sample_songs_from_chunks(data_chunks, 'rap')\n","\n","# Reset data chunks to read from the beginning\n","data_chunks = pd.read_csv(file_path, chunksize=chunk_size)\n","pop_songs = sample_songs_from_chunks(data_chunks, 'pop')\n"]},{"cell_type":"markdown","metadata":{"id":"eZnAtsK_dop3"},"source":["## PreProcessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"40_z9Hjcdop4"},"outputs":[],"source":["stopChars = [',','(',')','.','-','[',']','\"', '{', '}']\n","\n","def preprocessText(text):\n","    processedText = text.lower()\n","    processedText = re.sub(r'[^a-zA-Z\\s\\.,;!?\"\\'\\[\\]]', '', processedText)\n","    processedText = re.sub(r'\\([^()]*\\)|\\[[^\\[\\]]*\\]|\\{[^{}]*\\}', '', processedText)\n","    for char in stopChars:\n","        processedText = processedText.replace(char,'')\n","\n","    return processedText"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MGq4vD_Edop4","outputId":"f5e7c444-8722-4e30-a649-5f5ecc608010"},"outputs":[{"output_type":"stream","name":"stdout","text":["(5000, 2)\n","(5000, 2)\n"]}],"source":["rap_songs['lyrics']= rap_songs['lyrics'].apply(preprocessText)\n","pop_songs['lyrics']= pop_songs['lyrics'].apply(preprocessText)\n","print(rap_songs.shape)\n","print(pop_songs.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UHhGbU2Vdop5","outputId":"92bd75d3-3684-44e6-9012-04527b9f2ae4"},"outputs":[{"output_type":"stream","name":"stdout","text":["rap songs length: 3333625\n","pop songs length: 1903963\n","Rap Total Words: 64869\n","Pop Total Words: 25767\n"]}],"source":["def clean_songs(songs_data):\n","    # Perform substitution\n","    lyrics_corpus = re.sub(r'\\n|!|\\?', lambda x: ' ' + x.group(0) + ' ', songs_data['lyrics'].str.cat(sep='\\n').lower())\n","\n","    # Split the processed text into words and newlines\n","    lyrics_corpus = re.findall(r'\\S+|\\n', lyrics_corpus)\n","    garbage_words = [\"verse\", \"intro\", \"chorus\", \"bridge\", \"hook\", \"interlude\", \"outro\", \"prechorus\", \"postchorus\", \"instrumental\"]\n","\n","    # Remove garbage_words from the lyrics_corpus\n","    lyrics_corpus = [word for word in lyrics_corpus if word not in garbage_words]\n","\n","\n","    # Remove lines with two or fewer non-newline words\n","    filtered_corpus = []\n","    line = []\n","    word_count = 0\n","    for word in lyrics_corpus:\n","        if word == '\\n':\n","            if word_count > 2:\n","                filtered_corpus.extend(line)\n","                filtered_corpus.append('\\n')  # Keep the newline\n","            if word_count == 0:\n","                filtered_corpus.append('\\n')  # Keep the newline\n","            line = []\n","            word_count = 0\n","        else:\n","            line.append(word)\n","            word_count += 1\n","\n","    # If the last line has more than 2 words, add it to the filtered corpus\n","    if word_count > 2:\n","        filtered_corpus.extend(line)\n","        filtered_corpus.append('\\n')  # Keep the newline\n","\n","    return filtered_corpus\n","\n","cleaned_rap_songs = clean_songs(rap_songs)\n","cleaned_pop_songs = clean_songs(pop_songs)\n","\n","print('rap songs length:', len(cleaned_rap_songs))\n","print('pop songs length:', len(cleaned_pop_songs))\n","\n","# Counting characters appeared in all lyrics\n","words_rap = sorted(list(set(cleaned_rap_songs)))\n","words_pop = sorted(list(set(cleaned_pop_songs)))\n","print('Rap Total Words:', len(words_rap))\n","print('Pop Total Words:', len(words_pop))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9kujD9q2dop5","outputId":"f7a4b2c6-c63f-4100-ad96-2bd108f8e831"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total Words: 90636\n"]}],"source":["words = words_rap + words_pop\n","print('Total Words:', len(words))\n","\n","word_to_int = dict((w, i) for i, w in enumerate(words))\n","int_to_word = dict((i, w) for i, w in enumerate(words))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EJoS0n0gdop5","colab":{"base_uri":"https://localhost:8080/"},"outputId":"18b4fdc9-0af6-4cf1-9c6f-3cb28ebb09b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence Window:\n","[['\\n' 'show' 'a' 'big' 'timer' 'love' 'when' 'you' 'see' 'me']\n"," ['show' 'a' 'big' 'timer' 'love' 'when' 'you' 'see' 'me' 'riding']\n"," ['a' 'big' 'timer' 'love' 'when' 'you' 'see' 'me' 'riding' 'dubs']\n"," ['big' 'timer' 'love' 'when' 'you' 'see' 'me' 'riding' 'dubs' '\\n']\n"," ['timer' 'love' 'when' 'you' 'see' 'me' 'riding' 'dubs' '\\n' 'platinum']]\n","Target Words:\n","['riding' 'dubs' '\\n' 'platinum' 'chain']\n","Number of sequences: 3333615\n"]}],"source":["def create_sentences(lyrics, cleaned_corpus, seq_length, step):\n","    sentences = []\n","    next_words = []\n","    seq_length = 10 # The sentence window size\n","    step = 1 # The steps between the windows\n","    sentences = []\n","    next_words = []\n","\n","    # Create Target and sentences window\n","    for i in range(0, len(cleaned_corpus) - seq_length, step):\n","        # range from current index to sequence length words\n","        sentences.append(cleaned_corpus[i: i + seq_length])\n","        next_words.append(cleaned_corpus[i + seq_length]) # the next character\n","\n","    return sentences, next_words\n","\n","rap_sentences, rap_next_words = create_sentences(cleaned_rap_songs, cleaned_rap_songs, seq_length=10, step=1)\n","pop_sentences, pop_next_words = create_sentences(cleaned_pop_songs, cleaned_pop_songs, seq_length=10, step=1)\n","\n","rap_sentences = np.array(rap_sentences)\n","rap_next_words = np.array(rap_next_words)\n","\n","pop_sentences = np.array(pop_sentences)\n","pop_next_words = np.array(pop_next_words)\n","\n","# Print sentence window and next words\n","print('Sentence Window:')\n","print (rap_sentences[:5])\n","print('Target Words:')\n","print (rap_next_words[:5])\n","print('Number of sequences:', len(rap_sentences))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gh7FeAu5dop6"},"outputs":[],"source":["# transferring the word to index\n","def getdata(sentences, genre, next_words, seq_len=10):\n","    x = np.zeros((len(sentences),seq_len))\n","    g = np.zeros((len(sentences),seq_len))\n","    y = np.zeros((len(sentences)))\n","    length = len(sentences)\n","    index = 0\n","    for i in range(len(sentences)):\n","        sentence = sentences[i]\n","        for t, word in enumerate(sentence):\n","            x[i, t] = word_to_int[word]\n","            g[i, t] = genre\n","        y[i] = word_to_int[next_words[i]]\n","    return x, g, y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BeHjKWEfb1Z8"},"outputs":[],"source":["def encode_genre(genre):\n","    return 0 if genre == 'rap' else 1  # Encode 'rap' as 0 and 'pop' as 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZWCO8Ocvdop6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1794c017-20e4-4f6d-a32a-74a418575a19"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tensors:\n","train_x: [[64869. 84979. 65040. ... 90486. 84461. 78726.]\n"," [84979. 65040. 67095. ... 84461. 78726. 83537.]\n"," [65040. 67095. 58030. ... 78726. 83537. 71704.]\n"," ...\n"," [90486. 68228. 84461. ... 65008. 87482. 70482.]\n"," [68228. 84461. 64869. ... 87482. 70482. 90486.]\n"," [84461. 64869. 90490. ... 70482. 90486. 71005.]]\n","train_y: [83537. 71704. 64869. ... 90486. 71005. 64869.]\n","train_g: [[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [1. 1. 1. ... 1. 1. 1.]\n"," [1. 1. 1. ... 1. 1. 1.]\n"," [1. 1. 1. ... 1. 1. 1.]]\n","\n","Tensors Shapes:\n","Shape of train_x: (5237568, 10)\n","Shape of train_g: (5237568, 10)\n","Shape of train_y: (5237568,)\n"]}],"source":["train_x_RAP, train_g_RAP, train_y_RAP = getdata(rap_sentences, 0, rap_next_words)\n","train_x_POP, train_g_POP, train_y_POP = getdata(pop_sentences, 1, pop_next_words)\n","\n","train_x = np.concatenate([train_x_RAP, train_x_POP])\n","train_y = np.concatenate([train_y_RAP, train_y_POP])\n","train_g = np.concatenate([train_g_RAP, train_g_POP])\n","\n","print('Tensors:')\n","print('train_x:', train_x)\n","print('train_y:', train_y)\n","print('train_g:', train_g)\n","\n","print('\\nTensors Shapes:')\n","print('Shape of train_x:', train_x.shape)\n","print('Shape of train_g:', train_g.shape)\n","print('Shape of train_y:', train_y.shape)"]},{"cell_type":"markdown","metadata":{"id":"kCrpzRuJdop6"},"source":["## LSTM Network Definition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F8Cg3j1Kdop6"},"outputs":[],"source":["class Word_LSTM(nn.Module):\n","    def __init__(self, n_vocab, hidden_dim, embedding_dim, num_genres=2, genre_embedding_dim=128, dropout=0.4):\n","        super(Word_LSTM, self).__init__()\n","\n","        self.hidden_dim = hidden_dim\n","        self.embedding_dim = embedding_dim\n","        self.genre_embedding_dim = genre_embedding_dim\n","        self.genre_embedding = nn.Embedding(num_genres, genre_embedding_dim)\n","        self.word_embeddings = nn.Embedding(n_vocab, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim + genre_embedding_dim, hidden_dim, dropout=dropout, num_layers=2)\n","        self.fc = nn.Linear(hidden_dim, n_vocab)\n","\n","    def forward(self, seq_in, genre_in):\n","        # Embedding layer for words\n","        embedded_words = self.word_embeddings(seq_in.t())\n","\n","        # Embedding layer for genre\n","        genre_embedded = self.genre_embedding(genre_in.t())\n","\n","\n","        # Concatenate word embeddings with genre embeddings along the feature dimension\n","        concatenated = torch.cat((embedded_words, genre_embedded), dim=2)\n","\n","        # LSTM layer\n","        lstm_out, _ = self.lstm(concatenated)\n","\n","        # Last hidden state\n","        ht = lstm_out[-1]\n","\n","        # Fully connected layer\n","        out = self.fc(ht)\n","\n","        return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cUvuXmVKdop7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bd1b56d6-aee5-4664-8f46-04d8cc341d0d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of training_x: torch.Size([5237568, 10])\n","Shape of training_y: torch.Size([5237568])\n","Shape of training_genre: torch.Size([5237568, 10])\n"]}],"source":["# Convert numpy arrays to PyTorch tensors\n","X_train_tensor = torch.tensor(train_x, dtype=torch.long)\n","Y_train_tensor = torch.tensor(train_y, dtype=torch.long)\n","G_train_tensor = torch.tensor(train_g, dtype=torch.long)  # Genre tensor\n","\n","print('Shape of training_x:', X_train_tensor.shape)\n","print('Shape of training_y:', Y_train_tensor.shape)\n","print('Shape of training_genre:', G_train_tensor.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Po6-l_NFdop7"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","batch_size = 2048\n","\n","# Split data into training and validation sets\n","X_train, X_val, Y_train, Y_val, G_train, G_val = train_test_split(train_x, train_y, train_g, test_size=0.3, random_state=42)\n","\n","# Convert to PyTorch tensors\n","X_train_tensor = torch.tensor(X_train, dtype=torch.long)\n","Y_train_tensor = torch.tensor(Y_train, dtype=torch.long)\n","G_train_tensor = torch.tensor(G_train, dtype=torch.long)\n","X_val_tensor = torch.tensor(X_val, dtype=torch.long)\n","Y_val_tensor = torch.tensor(Y_val, dtype=torch.long)\n","G_val_tensor = torch.tensor(G_val, dtype=torch.long)\n","\n","# Create PyTorch datasets\n","train_dataset = torch.utils.data.TensorDataset(X_train_tensor, Y_train_tensor, G_train_tensor)\n","val_dataset = torch.utils.data.TensorDataset(X_val_tensor, Y_val_tensor, G_val_tensor)\n","\n","# Data loaders for training and validation sets\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n","# Define the model, optimizer, and criterion\n","model = Word_LSTM(n_vocab=len(words), hidden_dim=256, embedding_dim=256)\n","optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)  # Using Adam optimizer\n","criterion = nn.CrossEntropyLoss()\n"]},{"cell_type":"markdown","metadata":{"id":"cNztMGmSdop7"},"source":["## Training Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oHd6Csgbdop7","colab":{"base_uri":"https://localhost:8080/","height":373},"outputId":"044c5667-8257-4f3c-c40d-19a89f013b05"},"outputs":[{"output_type":"stream","name":"stderr","text":[" 85%|████████▌ | 1531/1791 [24:50<04:13,  1.03it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-3681f7eed740>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["n_epochs = 20\n","avg_losses_train = []\n","avg_losses_val = []\n","\n","for epoch in range(n_epochs):\n","    # Training\n","    model.train()\n","    avg_loss_train = 0.\n","\n","    for x_batch, y_batch, g_batch in tqdm(train_loader):\n","        y_pred = model(x_batch, g_batch)\n","        loss = criterion(y_pred, y_batch)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        avg_loss_train += loss.item() / len(train_loader)\n","\n","    avg_losses_train.append(avg_loss_train)\n","\n","    # Validation\n","    model.eval()\n","    avg_loss_val = 0.\n","\n","    with torch.no_grad():\n","        for x_batch, y_batch, g_batch in tqdm(val_loader):\n","            y_pred = model(x_batch, g_batch)\n","            loss = criterion(y_pred, y_batch)\n","\n","            avg_loss_val += loss.item() / len(val_loader)\n","\n","    avg_losses_val.append(avg_loss_val)\n","\n","    print(f'Epoch {epoch + 1}/{n_epochs}, Train Loss: {avg_loss_train:.4f}, Val Loss: {avg_loss_val:.4f}')\n"]},{"cell_type":"markdown","metadata":{"id":"zIFMlsb5cNjs"},"source":["## Train Loss plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iX78W_Eydop7"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Plotting\n","plt.plot(range(1, n_epochs + 1), avg_losses_train, label='Train Loss')\n","plt.plot(range(1, n_epochs + 1), avg_losses_val, label='Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WVJo1Rq1dop8"},"outputs":[],"source":["# Save model and dictionaries\n","torch.save({\n","    'model_state_dict': model.state_dict(),\n","    'word_to_int': word_to_int,\n","    'int_to_word': int_to_word\n","}, '/content/drive/MyDrive/Final_Project/Models/model.pt')\n"]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":2104038,"sourceId":3507350,"sourceType":"datasetVersion"}],"dockerImageVersionId":30683,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}